---
title: "Predictive Modeling - Concrete Compressive Strength"
subtitle: Rob Leonard (robleonard@tamu.edu)
output: html_document
  
---

# Concrete Data
For this assignment, build a predictive model that estimates the compressive strength of concrete using some of the methods learned to date. Identify and discuss the important features in the model.  

**Load Packages**
```{r pack setup, message = FALSE}
packs = c("dplyr","readxl","car","caret","glmnet","corrplot","earth","vip","randomForest","ranger","doParallel","xgboost","reshape2", "cowplot", "MASS" ,"ggplot2", "GGally")
lapply(packs, require, character.only = TRUE)
```
  
# Exploratory Data Analysis  
It should be standard practice to look at your dataset intensively before attempting to use it. The goals of EDA are:  
- Find any issues (missing data, extreme observations)  
- Gain insight into the types of transformations and methods that might be of use  
- Discover interesting things about the problem you're ultimately trying to solve
  
**Data Load, Exploration and Pre Processing Steps**  
```{r data load, include = FALSE}
Concrete_Data <- read_excel("Concrete_Data.xls")
``` 

## General Data Overview  
**Review data structure**  
```{r pairs, echo = FALSE, warning = FALSE, message = FALSE}
# Dimension 
cat("Number of Observations:",paste(dim(Concrete_Data)[1]),"\n Number of Features:",paste(dim(Concrete_Data)[2]-1))
# Check feature data types
cat("\n \n Data types:")
table(sapply(Concrete_Data[1,],class))
# Check for Missing Values
cat("\n Check for any missing values:",anyNA(Concrete_Data),"\n")
# Check for duplicate observations
concreteDataClean <- Concrete_Data %>% distinct()
varNames = c("Cement","BlastFurnaceSlag","FlyAsh","Water","Superplasticizer","CoarseAgg","FineAgg","Age","CompStr")
names(concreteDataClean) = varNames
# Check for duplicate observations within features only, take mean supervisor
# concreteDataClean %>% group_by(concreteDataClean[,1:8]) %>% filter(n()>1)) # 9 duplicated 
# Remove duplicates and use mean of compressive strength
concreteDataClean = concreteDataClean %>% group_by(Cement,BlastFurnaceSlag,FlyAsh,Water,Superplasticizer,CoarseAgg,FineAgg,Age) %>% summarise_all(mean)
cat("Number of Duplicate Observations:",paste(dim(Concrete_Data)[1]-dim(concreteDataClean)[1]))
```
There are 34 duplicate observations out of 1030. From the description it is unclear whether or not these are true duplicates but the descriptions lead towards them being duplicates and removing them. Further investigation would be required to check whether or not these are true duplicates.  

**Review Skewness and Transformations**
```{r Kernel Density, echo = FALSE}
# calc skewness coefficient
skewnessVec = round(concreteDataClean[,1:8] %>% sapply(., e1071::skewness, na.rm = TRUE),3)
label = "Skewness:"
# plot kernel densities of the features
par(mfrow=c(3,3))
for (i in 1:8) {
   plot(density(pull(concreteDataClean, var = i), kernel = "gaussian", bw="nrd"), type="l", xlab=bquote(.(label)~.(skewnessVec[i])), main = colnames(concreteDataClean[i]))
}
```
  
While age is highly right skewed, due to it's lumpiness, a transformation isn't likely to be useful. There are also some extreme observations in age. Tree methods look like they might be useful due to several features being bimodal like blastfurnaceslag, flyash and superplasticizer.  
  
**Check for Outliers**  
```{r PCA Outliers, echo = FALSE}
pcaModel = prcomp(concreteDataClean, center = TRUE, scale = TRUE)
plot(pcaModel$x[,1:2], xlim = c(-8,8), ylim = c(-8,8), main="Plot of First 2 PC Scores")  
```
  
There don't appear to be any extreme points.  

**Multicollinearity Check**  
```{r CorrPlots, echo = FALSE}
corrplot(cor(concreteDataClean), order = "hclust", tl.cex = .75)  
```
  
High multicollinearity is not present.  

# Modeling  
**Training and Test Data Split**    
```{r Data Splitting}
set.seed(1999)
yAllData = concreteDataClean$CompStr
xAllData = as.data.frame(concreteDataClean[,-9])
trainIndex = createDataPartition(yAllData, p=.7, list = FALSE) %>% as.vector(.)
yTrain = yAllData[trainIndex]
xTrain = xAllData[trainIndex,]
yTest = yAllData[-trainIndex]
xTest = xAllData[-trainIndex,]  
```
A 70%/30% training/testing split is applied to the dataset.  


**ScatterPlots**  
```{r FeaturePlots, echo=FALSE}
featurePlot(xTrain, yTrain, type = c("g","p","smooth"), col.line = "black")
```
  
Several features look like they might need a quadratic term as the local fit isn't a straight line.  

**Feature Engineering**  
```{r Transformations}
xTrain = xTrain %>% mutate(Cementsq = Cement^2, BlastFurnaceSlagSq = BlastFurnaceSlag^2, FlyAshSq = FlyAsh^2, WaterSq = Water^2, SuperplasticizerSq = Superplasticizer^2, CoarseAggSq = CoarseAgg^2, FineAggSq = FineAgg^2, AgeSq = Age^2)
xTest = xTest %>% mutate(Cementsq = Cement^2, BlastFurnaceSlagSq = BlastFurnaceSlag^2, FlyAshSq = FlyAsh^2, WaterSq = Water^2, SuperplasticizerSq = Superplasticizer^2, CoarseAggSq = CoarseAgg^2, FineAggSq = FineAgg^2, AgeSq = Age^2) 
# create matrix forms for some models
xTrainMat = as.matrix(xTrain)
xTestMat = as.matrix(xTest)
```
Given the limited number of features and the size of the dataset, it's feasible to add quadratic terms as identified above.  

**Setup train control**  
```{r Train Control}
trControl = trainControl(method = "repeatedcv", repeats = 2, number = 10)
```
  
**Set up parallel processing**    
```{r Parallels}
cl = makeCluster(20)
registerDoParallel(cl)
```
  
## Linear Modeling  
**Elastic Net**  
```{r Elastic Net, echo = FALSE, cache = TRUE}
lassoGrid = expand.grid(lambda = c(0.0001, 0.001, .001, .01, .05, .1), alpha = c(.05,.25,.5,.75,1))
elasticOut = train(x = xTrainMat, y = yTrain, method = "glmnet", preProc = c("center", "scale"), tuneGrid = lassoGrid, trControl = trControl)
plot(elasticOut)
```
  
RMSE bottoms out just below 8 in terms of mean compressive strength.  
  
**Model Validation**
```{r EN Results, echo = FALSE, cache = TRUE}
cat("Elastic Net - Best Model Tuning Parameters:\n")
elasticOut$bestTune

glmnetOut = glmnet(x = xTrainMat, y = yTrain, alpha = elasticOut$bestTune$alpha)
betaHatGlmnet = coef(glmnetOut, s = elasticOut$bestTune$lambda)
yHatTrainGlmnet = predict(glmnetOut, xTrainMat, s = elasticOut$bestTune$lambda)

residuals = yTrain - yHatTrainGlmnet
plot(yHatTrainGlmnet, residuals, xlab = "Training Predictions", ylab = "Residuals")
abline(h=0, col="gray")
```
  
The residuals look appropriate.  They are a random scatter around the horizontal axis.  The chosen EN model is skipped here as nonlinear models are likely to provide better predictions as previously discussed.  

  
## Nonlinear Models  
**MARS**    
```{r MARS, echo = FALSE, cache = TRUE}
tuneGrid = expand.grid(degree = 1:3, nprune = seq(from = 10, to = 40, by = 5))
marsOut = train(x = xTrain, y = yTrain, method = "earth", tuneGrid = tuneGrid, trControl = trControl)
plot(marsOut)
cat("MARS - Best Model Tuning Parameters:\n")
marsOut$bestTune
```
  
The best MARS model includes quadratic terms.  RMSE has been reduced to under 6.  

**SVM**   
```{r SVM Radial, echo = FALSE, cache = TRUE}
pairWiseDist = dist(scale(xTrain), method = 'euclidean')**2
sigmaRange = quantile(pairWiseDist, c(0.9, 0.5, 0.1))
tuneGrid = expand.grid(C = c(.01, .1, 1, 10, 50), sigma = round(1/sigmaRange,4))
svmOut = train(x = xTrain, y = yTrain, method = "svmRadial", tuneGrid = tuneGrid, preProc = c("center","scale"), trControl = trControl)
plot(svmOut)
```
  
A support vector machine does not perform better than MARS, although there are numerous tuning paramters that might be adjusted to increase performance.    
  
**Random Forest**  
```{r Random Forest, echo = FALSE, warning = FALSE, cache = TRUE}
# set up a search grid of tuning parameters
nTree = c(100,250,500,750,1000,1250,1500,2000)
nodeSize = c(4,5,6)
rfTune = matrix(0, nrow = length(nTree), ncol = length(nodeSize))

for (i in 1:length(nTree) ) {
  for (j in 1:length(nodeSize)) {
    tuneGridRanger = data.frame(splitrule = 'variance', min.node.size = nodeSize[j], mtry = round(sqrt(ncol(xTrain))))
    rfTuneOut      = train(x = xTrain, y = yTrain, method = "ranger", num.trees = nTree[i], tuneGrid = tuneGridRanger,
                   importance = 'permutation', trControl = trControl)
    rfTune[i,j] = rfTuneOut$results$RMSE
  }
}
# Rerun with best tuning parameters
bestRFIndex = which(rfTune == min(rfTune), arr.ind = TRUE)
tuneGridRanger = data.frame(splitrule = 'variance', min.node.size = nodeSize[bestRFIndex[2]], mtry = round(sqrt(ncol(xTrain))))
rfOut      = train(x = xTrain, y = yTrain, method = "ranger", num.trees = nTree[bestRFIndex[2]], tuneGrid = tuneGridRanger,
                   importance = 'permutation', trControl = trControl)
# Plot Results
rfTunePlot = data.frame(nTree = rep(nTree,3), RMSE = as.vector(rfTune), mtry = c(rep("4",8),rep("5",8),rep("6",8)))

ggplot(rfTunePlot, aes(x=nTree, y=RMSE, group = mtry)) + geom_line(aes(color=mtry)) + geom_point(aes(color=mtry)) + geom_point(aes(shape=mtry)) + theme_classic() + scale_y_continuous(limits = c(5.3,5.6)) + scale_x_continuous(breaks = c(seq(250,2000,250)))
```
  
Random Forest improves the estimated predictive performance. RMSE is well below 6.    
   
**Boosting**    
```{r Boosting, echo = FALSE, cache = TRUE}
tuneGrid= data.frame('nrounds' = c(50,150,300,500,1000,1500,2000), 'max_depth' = 6, 'eta' = .05, 'gamma' = 0, 'colsample_bytree' = 1, 'min_child_weight' = 0, 'subsample' = .5)
boostOut = train(x = xTrainMat, y = yTrain, method = 'xgbTree', verbose = 0, tuneGrid = tuneGrid, trControl = trControl)
plot(boostOut)
boostImportance = xgb.importance(model = boostOut$finalModel)
cat("Boosting - Best Model Tuning Paramters:")
boostOut$bestTune
```
  
Boosted trees significantly improves performance. This is the best model so far.   
  
**Neural Network**  
```{r NeuralNet, echo = FALSE, cache = TRUE}
nnGrid = expand.grid(.decay = c(.001,.01,.1), .size = seq(from = 1, to = 38, by = 3), .bag = FALSE)
nnOut = train(y = yTrain, x = xTrain, method = "avNNet", tuneGrid = nnGrid, preProc=c("center","scale"), linout = TRUE, trace = FALSE, maxit = 1000, trControl = trControl)

plot(nnOut)
cat("Neural Network - Best Model Tuning Paramters:")
nnOut$bestTune
```

A neural network also show promising results.  However, computational time was significantly longer than boosted trees.


# Judging Performance  
```{r Estimated Test Error, include = FALSE, cache = TRUE}
yHatGlmnet = c(predict(glmnetOut, xTestMat,  s = elasticOut$bestTune$lambda))
yHatMARS   = c(predict(marsOut, xTest))
yHatSVM    = predict(svmOut, xTest)
yHatRF     = predict(rfOut, xTest)
yHatBoost  = predict(boostOut, xTestMat)
yHatNN     = predict(nnOut, xTest)
resultsTest = data.frame(cbind(yHatGlmnet,yHatMARS,yHatSVM,yHatRF,yHatBoost,yHatNN))

resultsRMSE = data.frame(Method = c("(1) ElasticNet","(2) MARS","(3) SVM","(4) RF","(5) Boost","(6) NeuralNet"), RMSE = sapply(resultsTest,function(yHat){sqrt(mean((yHat - yTest)^2))}))
```

```{r Results, echo = FALSE}
ggplot(data = resultsRMSE, aes(x=Method, y=RMSE, fill=Method)) + geom_bar(stat="identity", show.legend = FALSE) + theme_classic() + scale_y_continuous(limits = c(0,10), breaks = seq(0,10,2)) 
```
  
The nonlinear methods significantly outperformed the linear model. Boosted trees provided the best performance.
    
**Variable Importance**  
```{r VIP, echo=FALSE}
xgb.plot.importance(boostImportance)
```
  
Age and cement are the two most important features in predicting compressive strength. Interpreting each feature's contribution to a prediction is a little more difficult with boosted trees than with linear models. However, the feature waterfall chart for observation 20 is shown, and provides an idea for how much each feature contributes to the estimated compressive strength. This can be applied to any observation of interest.  Individual feature plots can also be extracted, and the final plot shows the effect of age on compressive strength, with the effects leveling off after 100 days. Concrete mixture samples younger than 100 days have a decrease in estimated compressive strength.  One further item to note is that these observations really are mixtures. Further investigation should consider mixture modeling but is beyond the scope of this course.  
   

```{r Prediction Explainer, cache = TRUE, message = FALSE}
library(xgboostExplainer)
xTrainDM = xgb.DMatrix(data = xTrainMat, label = yTrain)
xTestDM = xgb.DMatrix(data = xTestMat)
boostExplainer = buildExplainer(boostOut$finalModel, xTrainDM, type = "regression")
predExplained = explainPredictions(boostOut$finalModel, boostExplainer, xTestDM)
showWaterfall(boostOut$finalModel, boostExplainer, xTestDM, xTestMat,20, type = "regression")
plot(xTestMat[,"Age"], predExplained[,Age])
```













